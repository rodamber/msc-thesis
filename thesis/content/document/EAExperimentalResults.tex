\section{Experimental Results}
\label{sec:experimental-results}

\begin{table*}[t]
  \footnotesize
  \noindent\makebox[\textwidth]{
    \begin{tabular}{ccccc}
      \toprule

      \multicolumn{1}{l}{Configuration}
      & \multicolumn{1}{l}{\# Instances solved}
      & \begin{tabular}[c]{@{}c@{}}\# Instances solved\\ (matching expected\\ solution)\end{tabular}
      & Mean Time (s)
      & \multicolumn{1}{l}{Median Time (s)}
      \\ \midrule

      S-e1:c1    & 48 & 3  & 4.62  & 0.48 \\ \midrule 
      S-e2:c1    & 39 & 21 & 51.28 & 3.21 \\ \midrule
      S-e3:c1    & 34 & 23 & 51.38 & 4.47 \\ \midrule
      W-e1:c1    & 47 & 2  & 6.76  & 0.65 \\ \midrule
      W-e2:c1    & 13 & 4  & 11.14 & 2.24 \\ \midrule
      W-e3:c1    & 9  & 5  & 21.45 & 7.48 \\ \midrule
      SyPet-All  & 11 & 6  & 31.27 & 3.00 \\ \midrule
      SyPet-User & 32 & 30 & 52.75 & 5.00 \\ \bottomrule

    \end{tabular}}
  \caption{Comparison between the different configurations by number of instances
    solved and running wall-clock time for solved instances (not necessarily
    matching the expected solution).}
  \label{table:comparison-configs}
\end{table*}


A set of 285,522 expressions were provided by OutSystems.
We conducted an analysis to determine which builtin functions and which
combinations were the most common in that set.
We picked 51 expressions containing only functions from
Table~\ref{table:builtin-description}, and sizes (number of components) ranging
from 1 to 7.
The hardness of a benchmark depends on the size of the solution, the number of
input-output examples, and the library of components.
Typically, the higher the size and the number of components, the harder it is to
synthesize a program.
We obtained a set of 3 input-output examples for each of these 51 expressions.
In order to do that, we developed an \textit{interpreter} for OutSystems
expressions, and manually created a set of 3 different inputs for each
expression.
The inputs were carefully crafted in order to try to eliminate as much ambiguity
as possible.
Then we interpreted the expressions over their set of inputs in order to
obtain the corresponding outputs.

We used Z3 version 4.8.5~\cite{DeMoura:2008:ZES} as the underlying \gls{smt}
solver. 
Each benchmark was monitored by the \textit{runsolver}
program~\cite{Roussel:2011:JSAT}, and restricted to a wall-clock time limit of
600 seconds, and a memory limit of 16 GB.
We ran the experiments in an computer with Intel(R) Xeon(R) CPU E5-2620 v4 @
2.10GHz processors.

\subsection{Evaluation}
\label{sec:eval}

We are interested in the relation between the number and quality of the
input-output examples, and their impact on synthesis time and program quality.
In this section, we only study the impact of the number of input-output
examples.

We present the results for both synthesizers with the following configurations.
For each instance of 3 input-output examples we ran both synthesizers using 1,
2, or all 3 of the input-output examples.
We ran the Setwise synthesizer configured
to synthesize programs with a maximum of one integer and one string constants
(configurations S-e1:c1, S-e2:c1, and S-e3:c1, for 1, 2, and 3 input-output
examples, respectively).
We ran the Whole synthesizer configured to
synthesize programs with a maximum of one constant, which the synthesizer can
choose whether it is a an integer or a string (configurations W-e1:c1, W-e2:c1,
and W-e3:c1, for for 1, 2, and 3 input-output examples, respectively).
Ideally, we would also have tried other configurations to see the impact of
using a different number of constants.
For comparison, we also instantiated SyPet~\cite{Feng:2017:CSC}, a
\gls{pbe} component-based synthesizer for Java programs that employs a
type-directed search together with a constraint-based technique, with our
library of components.
However, SyPet does not support guessing the value of constants.
With this in mind, we set up two configurations for SyPet.
For the first configuration (SyPet-All), we introduced a new 0-arity component
for each constant in a pool of predefined constants.
For the second configuration (SyPet-User), we looked at each particular
instance, and introduced new components only for the constants needed for the
expected solution.
This mimics a setting where the constants are declared by the user, instead of
being guessed by the synthesizer.
Both Sypet-User and Sypet-All are configured for using all three examples of
each benchmark, and may use as many constants as they need (out of the available
ones).
Table~\ref{table:comparison-configs} shows a comparison between the different
configurations by number of instances solved and running wall-clock time (mean
and median for solved instances).
An instance is considered solved if the synthesized program satisfies the
input-output examples (but might or might not generalize).
Matching the expected solution means, that the synthesizer outputs a program
that, not only satisfies the input-output examples, but also captures the
original intent and generalizes to more examples.

% Expected comparison
\begin{figure*}
  \centering
  \includegraphics[scale=0.7]{assets/comparison-expected-sypet-small.pdf}
  \caption{Number of solved instances matching the expected solution per size of
    the expected solution for the Setwise (one integer constant and one string
    constant) and Whole synthesizers (one constant) with three examples, and
    both SyPet configurations (user-provided constants and pool of constants).}
  \label{fig:comparison-expected-sypet}
\end{figure*}

\subsection{Discussion}
\label{sec:discussion}

Figure~\ref{fig:comparison-expected-sypet} shows a comparison between all
configurations on three input-output examples.
In Figure~\ref{fig:comparison-expected-sypet} we can see that SyPet-All has
similar results to W-e3:c1.
The large number of components makes the search space intractable.
Besides, SyPet cannot take advantage of its type-directed algorithm because
we are dealing with only two types (integers and strings).
This last point also applies to SyPet-User, but the advantage given by the
user-provided constants proves crucial for its fairly good results, which are
actually better than S-e3:c1 (Table~\ref{table:comparison-configs}).
Many expected solutions require more than one constant, with some requiring more
than one constant of each type.
This helps explain the poor performance of the Whole synthesizer.
However, it could be speculated from the start that the Whole synthesizer would
be less efficient than the Setwise synthesizer.
Typically, solving one large constraint is more expensive than solving multiple
smaller constraints.
Still, it would be interesting to benchmark configurations with more
constants, or with user-provided constants.

The running times for the solved benchmarks are reasonably fast, allowing for
tolerable interaction times with the user.
However, synthesizing programs with 4 or more lines seems to be out of reach for
these configurations (Figure~\ref{fig:comparison-expected-sypet}).
